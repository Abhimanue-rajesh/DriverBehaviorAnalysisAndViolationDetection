{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the custom model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a custom YOLOv5 model from the 'ultralytics/yolov5' repository with the 'custom' architecture.\n",
    "# The 'path' parameter specifies the path to the custom model weights file ('last.pt').\n",
    "# The 'force_reload' parameter forces reloading the model weights, ensuring that the latest version is loaded.\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='yolov5/runs/train/exp15/weights/last.pt', force_reload=True)\n",
    "# Change the path to the custom model that you have trained\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Test with an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the file path for an image using the os.path.join() function\n",
    "# The image is located in the 'data/images' directory.\n",
    "img = os.path.join('data', 'images', 'awake') # Change the awake to the name of you file that you want to check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib and configure it to display plots inline in Jupyter Notebook or Jupyter Lab\n",
    "%matplotlib inline\n",
    "\n",
    "# Display the rendered image using matplotlib\n",
    "# The image is obtained from the 'results' object and rendered using np.squeeze() to remove any unnecessary dimensions\n",
    "# Finally, plt.imshow() displays the image\n",
    "plt.imshow(np.squeeze(results.render()))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Test the model with realtime video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a connection to the default camera (index 0)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Continuously loop until the camera is opened\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video capture\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Make object detections on the current frame using the custom YOLOv5 model\n",
    "    results = model(frame)\n",
    "    \n",
    "    # Display the rendered image with detected objects using OpenCV\n",
    "    cv2.imshow('YOLO', np.squeeze(results.render()))\n",
    "    \n",
    "    # Check if the 'q' key is pressed to quit the video feed\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture device\n",
    "cap.release()\n",
    "\n",
    "# Close all OpenCV windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
